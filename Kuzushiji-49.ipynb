{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torchvision\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "class Kuzushijidataset():\n",
    "    \n",
    "    resources = [\n",
    "    (\"http://codh.rois.ac.jp/kmnist/dataset/k49/k49-train-imgs.npz\"),\n",
    "    (\"http://codh.rois.ac.jp/kmnist/dataset/k49/k49-train-labels.npz\"),\n",
    "    (\"http://codh.rois.ac.jp/kmnist/dataset/k49/k49-test-imgs.npz\"),\n",
    "    (\"http://codh.rois.ac.jp/kmnist/dataset/k49/k49-test-labels.npz\")]\n",
    "    \n",
    "    training_file_imgs = \"k49-train-imgs.npz\"\n",
    "    training_file_labels = \"k49-train-labels.npz\"\n",
    "    test_file_imgs = \"k49-test-imgs.npz\"\n",
    "    test_file_labels = \"k49-test-labels.npz\"\n",
    "    data_dir = \"data\"\n",
    "    \n",
    "   \n",
    "    def __init__(self, data_dir=\"data\", train=True, transform=None, download=True):\n",
    "        self.data_dir = data_dir\n",
    "        \n",
    "        if download:\n",
    "            self.download(train)\n",
    "            \n",
    "        train_data_imgs = os.path.join(self.data_dir, self.training_file_imgs)\n",
    "        train_data_imgs = np.load(train_data_imgs)\n",
    "        train_data_imgs = train_data_imgs.f.arr_0\n",
    "\n",
    "        train_data_labels = os.path.join(self.data_dir, self.training_file_labels)\n",
    "        train_data_labels = np.load(train_data_labels)\n",
    "        train_data_labels = train_data_labels.f.arr_0\n",
    "\n",
    "        test_data_imgs = os.path.join(self.data_dir, self.test_file_imgs)\n",
    "        test_data_imgs = np.load(test_data_imgs)\n",
    "        test_data_imgs = test_data_imgs.f.arr_0\n",
    "        \n",
    "        test_data_labels = os.path.join(self.data_dir, self.test_file_labels)\n",
    "        test_data_labels = np.load(test_data_labels)\n",
    "        test_data_labels = test_data_labels.f.arr_0\n",
    "        \n",
    "        self.transform = transform\n",
    "        \n",
    "        if train:\n",
    "            self.data = train_data_imgs\n",
    "            self.targets = train_data_labels\n",
    "        else:\n",
    "            self.data = test_data_imgs\n",
    "            self.targets = test_data_labels\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return(len(self.data))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img, target = self.data[idx], int(self.targets[idx])\n",
    "\n",
    "        # doing this so that it is consistent with all other datasets\n",
    "        # to return a PIL Image\n",
    "        img = Image.fromarray(img, mode='L') # mode='L' - (8-bit pixels, black and white)\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        return img, target        \n",
    "      \n",
    "    def download(self, train):\n",
    "        # download the Kuzushiji-49 dataset if it doesn't exist\n",
    "        if self._check_exists():\n",
    "            if train:\n",
    "                print('Train dataset already exists!')\n",
    "            else:\n",
    "                print('Test dataset already exists!')\n",
    "            return\n",
    "\n",
    "        if not os.path.exists(self.data_dir):\n",
    "            os.makedirs(self.data_dir)\n",
    "            \n",
    "        for url in self.resources:\n",
    "            filename = url.rpartition('/')[2]\n",
    "            print('Downloading: ', filename)\n",
    "            myfile = requests.get(url, allow_redirects=True)\n",
    "            open(os.path.join(self.data_dir, filename), 'wb').write(myfile.content)\n",
    "\n",
    "        print('All files downloaded!')\n",
    "        \n",
    "\n",
    "    def _check_exists(self):\n",
    "        return (os.path.exists(os.path.join(self.data_dir, self.training_file_imgs)) and\n",
    "                os.path.exists(os.path.join(self.data_dir, self.training_file_labels)) and\n",
    "                os.path.exists(os.path.join(self.data_dir, self.test_file_imgs)) and\n",
    "                os.path.exists(os.path.join(self.data_dir, self.test_file_labels)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['あ - a', 'い - i', 'う - u', 'え - e', 'お - o', \n",
    "       'か - ka', 'き - ki', 'く - ku', 'け - ke', 'こ - ko', \n",
    "       'さ - sa', 'し - shi', 'す - su', 'せ - se', 'そ - so', \n",
    "       'た - ta', 'ち - chi', 'つ - tsu', 'て - te', 'と - to', \n",
    "       'な - na', 'に - ni', 'ぬ - nu', 'ね - ne', 'の - no',\n",
    "       'は - ha', 'ひ - hi', 'ふ - fu', 'へ - he', 'ほ - ho',\n",
    "       'ま - ma', 'み - mi', 'む - mu', 'め - me', 'も - mo',\n",
    "       'や - ya', 'ゆ - yu', 'よ - yo', \n",
    "       'ら - ra', 'り - ri', 'る - ru', 'れ - re', 'ろ - ro',\n",
    "       'わ - wa', 'ゐ - wi', 'ゑ - we', 'を - wo', 'ん - n', 'ゝ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading:  k49-train-imgs.npz\n",
      "Downloading:  k49-train-labels.npz\n",
      "Downloading:  k49-test-imgs.npz\n",
      "Downloading:  k49-test-labels.npz\n",
      "All files downloaded!\n",
      "Test dataset already exists!\n",
      "Train dataset size:  232365\n",
      "Test dataset size:  38547\n"
     ]
    }
   ],
   "source": [
    "# Load train and test datasets\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "\n",
    "train_data = Kuzushijidataset(\"data\", train=True, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "test_data = Kuzushijidataset(\"data\", train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "\n",
    "print(\"Train dataset size: \", len(train_data))\n",
    "print(\"Test dataset size: \", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training dataset into training and validation\n",
    "train_data, val_data = torch.utils.data.random_split(train_data, [210000, 22365])    # split into 200k training & ~32k validation (roughly the size of the test dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "# create training data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True) \n",
    "\n",
    "\n",
    "# create validation data loader\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# create test data loader\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_data, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "へ - he | め - me | ひ - hi |     ゝ | し - shi |\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "qtd_images = 5\n",
    "\n",
    "# functions to show an image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images[:qtd_images], nrow = qtd_images, scale_each=False))\n",
    "# print labels\n",
    "print(' '.join('%5s |' % classes[labels[j]] for j in range(qtd_images)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "            # torch.nn.conv2d(in_channels, out_channels, kernel_size)\n",
    "            # in_channels is the number of layers which it takes in (i.e.num color channels in 1st layer)\n",
    "            # out_channels is the number of different filters that we use\n",
    "            # kernel_size is the depthxwidthxheight of the kernel#\n",
    "            # stride is how many pixels we shift the kernel by each time\n",
    "        self.conv_layers = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=1, out_channels=32, kernel_size=5, stride=1), # -4\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(in_channels=32, out_channels= 64, kernel_size=5, stride=1),#\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.fc_layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(64 * 20 * 20, 120),\n",
    "            torch.nn.Linear(120, 49)\n",
    "        )\n",
    "        \n",
    "        # Spatial transformer localization-network\n",
    "        self.localization = torch.torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 8, kernel_size=7),\n",
    "            torch.nn.MaxPool2d(2, stride=2),\n",
    "            torch.nn.ReLU(True),\n",
    "            torch.nn.Conv2d(8, 10, kernel_size=5),\n",
    "            torch.nn.MaxPool2d(2, stride=2),\n",
    "            torch.nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        # Regressor for the 3 * 2 affine matrix\n",
    "        self.fc_loc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(10 * 3 * 3, 32),\n",
    "            torch.nn.ReLU(True),\n",
    "            torch.nn.Linear(32, 3 * 2)\n",
    "        )\n",
    "\n",
    "        # Initialize the weights/bias with identity transformation\n",
    "        self.fc_loc[2].weight.data.zero_()\n",
    "        self.fc_loc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n",
    "\n",
    "        \n",
    "    def stn(self, x):\n",
    "        xs = self.localization(x)\n",
    "        xs = xs.view(-1, 10 * 3 * 3)\n",
    "        theta = self.fc_loc(xs)\n",
    "        theta = theta.view(-1, 2, 3)\n",
    "\n",
    "        grid = F.affine_grid(theta, x.size())\n",
    "        x = F.grid_sample(x, grid)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.stn(x)\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc_layers(x)\n",
    "        X = F.softmax(x, dim=1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN = ConvNet().to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter() # we will use this to show our models performance on a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs, optimiser='Adam', learning_rate = 0.0001, verbose=False, tag='Loss/Train'):\n",
    "    if(optimiser=='Adam'):\n",
    "        optimiser = torch.optim.Adam(model.parameters(), lr= learning_rate)\n",
    "    else:\n",
    "        optimiser = torch.optim.SGD(model.parameters(), lr= learning_rate)\n",
    "        \n",
    "    \n",
    "    model.train()                                  # put the model into training mode (more on this later)\n",
    "    for epoch in range(epochs):\n",
    "        for idx, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            prediction = model(inputs) # pass the data forward through the model\n",
    "            loss = criterion(prediction, labels) # compute the loss\n",
    "            optimiser.zero_grad() # reset the gradients attribute of each of the model's params to zero\n",
    "            loss.backward() # backward pass to compute and set all of the model param's gradients\n",
    "            optimiser.step() # update the model's parameters\n",
    "            writer.add_scalar(tag, loss, epoch*len(train_loader) + idx)    # write loss to a graph\n",
    "        if verbose: print('Epoch:', epoch, '\\tAccuracy:', calc_accuracy(model, val_loader), '\\tLoss:', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "            \n",
    "def calc_accuracy(model, dataloader, testset=False):\n",
    "    misclass_i = []\n",
    "    misclass_l = []\n",
    "    num_correct = 0\n",
    "    num_examples = len(dataloader.dataset)                       # test DATA not test LOADER\n",
    "    for inputs, labels in dataloader:                  # for all exampls, over all mini-batches in the test dataset\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        predictions = model(inputs)\n",
    "        predictions = torch.max(predictions, axis=1)    # reduce to find max indices along direction which column varies\n",
    "        predictions = predictions[1]                    # torch.max returns (values, indices)\n",
    "        num_correct += int(sum(predictions == labels))\n",
    "\n",
    "        if testset: #generate a list of misclassified data\n",
    "            if sum(predictions != labels):\n",
    "                for i in range(len(predictions)):\n",
    "                    if(predictions[i] != labels[i]):\n",
    "                        misclass_i.append(inputs[i])\n",
    "                        misclass_l.append(labels[i])\n",
    "                \n",
    "    percent_correct = num_correct / num_examples * 100\n",
    "    return percent_correct, misclass_i, misclass_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elisio/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:2751: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\"Default grid_sample and affine_grid behavior will be changed \"\n",
      "/home/elisio/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\"Default grid_sample and affine_grid behavior will be changed \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tAccuracy: (87.89179521573888, [], []) \tLoss: tensor(0.1699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tAccuracy: (91.11558238318803, [], []) \tLoss: tensor(0.3328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tAccuracy: (92.96668902302704, [], []) \tLoss: tensor(0.1340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tAccuracy: (93.83858707802369, [], []) \tLoss: tensor(0.3004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tAccuracy: (94.12027721886876, [], []) \tLoss: tensor(0.2140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tAccuracy: (94.6121171473284, [], []) \tLoss: tensor(0.1478, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tAccuracy: (94.77755421417393, [], []) \tLoss: tensor(0.0521, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tAccuracy: (94.87145092778896, [], []) \tLoss: tensor(0.0781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Train Accuracy: (97.55142857142857, [], [])\n"
     ]
    }
   ],
   "source": [
    "model = CNN\n",
    "\n",
    "train(model, epochs=8, optimiser='Adam', learning_rate=0.0001, verbose=True)\n",
    "print('Train Accuracy:', calc_accuracy(model, train_loader))\n",
    "print('Test Accuracy:', calc_accuracy(model, test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
