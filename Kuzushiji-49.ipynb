{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KuzushijiDataset(Dataset):\n",
    "    def __init__(self, root_dir, train=True, transform=None):\n",
    "        train_data_imgs = os.path.join(root_dir, \"k49-train-imgs.npz\")\n",
    "        train_data_imgs = np.load(train_data_imgs[0:70000])\n",
    "        train_data_imgs = train_data_imgs.f.arr_0\n",
    "\n",
    "        train_data_labels = os.path.join(root_dir, \"k49-train-labels.npz\")\n",
    "        train_data_labels = np.load(train_data_labels[0:70000])\n",
    "        train_data_labels = train_data_labels.f.arr_0\n",
    "        #self.train_data = [train_data_imgs, train_data_labels] # train dataset\n",
    "\n",
    "        test_data_imgs = os.path.join(root_dir, \"k49-test-imgs.npz\")\n",
    "        test_data_imgs = np.load(test_data_imgs)\n",
    "        test_data_imgs = test_data_imgs.f.arr_0\n",
    "        \n",
    "        test_data_labels = os.path.join(root_dir, \"k49-test-labels.npz\")\n",
    "        test_data_labels = np.load(test_data_labels)\n",
    "        test_data_labels = test_data_labels.f.arr_0\n",
    "        #self.test_data = [test_data_imgs, test_data_labels] # test dataset\n",
    "        \n",
    "        self.transform = transform\n",
    "        #self.train = train\n",
    "        \n",
    "        if train:\n",
    "            self.data = train_data_imgs\n",
    "            self.targets = train_data_labels\n",
    "        else:\n",
    "            self.data = test_data_imgs\n",
    "            self.targets = test_data_labels\n",
    "            \n",
    "        #print(self.data.shape)\n",
    "        #print(self.targets.shape)\n",
    "\n",
    "    def __len__(self):\n",
    "        return(len(self.data))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        img, target = self.data[idx], int(self.targets[idx])\n",
    "        #print(self.data[1].shape)\n",
    "        # doing this so that it is consistent with all other datasets\n",
    "        # to return a PIL Image\n",
    "        img = Image.fromarray(img, mode='L') # mode='L' - (8-bit pixels, black and white)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        #if self.transform is not None:\n",
    "        #    target = self.transform(target)\n",
    "            \n",
    "        \n",
    "\n",
    "        return img, target        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size:  232365\n",
      "Test dataset size:  38547\n"
     ]
    }
   ],
   "source": [
    "# Load train and test datasets\n",
    "train_data = KuzushijiDataset(\"data\", train=True, transform=transforms.ToTensor())\n",
    "\n",
    "test_data = KuzushijiDataset(\"data\", train=False, transform=transforms.ToTensor())\n",
    "\n",
    "#train_data = train_data[:70000]\n",
    "\n",
    "print(\"Train dataset size: \", len(train_data))\n",
    "print(\"Test dataset size: \", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training dataset into training and validation\n",
    "#train_data, val_data = torch.utils.data.random_split(train_data[:70000], [50000, 20000])    # split into 200k training & ~32k validation (roughly the size of the test dataset)\n",
    "train_data, val_data = torch.utils.data.random_split(train_data, [200000, 32365])    # split into 200k training & ~32k validation (roughly the size of the test dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "# create training data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True) \n",
    "\n",
    "\n",
    "# create validation data loader\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# create test data loader\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_data, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "            # torch.nn.conv2d(in_channels, out_channels, kernel_size)\n",
    "            # in_channels is the number of layers which it takes in (i.e.num color channels in 1st layer)\n",
    "            # out_channels is the number of different filters that we use\n",
    "            # kernel_size is the depthxwidthxheight of the kernel#\n",
    "            # stride is how many pixels we shift the kernel by each time\n",
    "        self.conv_layers = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=1, out_channels=32, kernel_size=5, stride=1), # -4\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(in_channels=32, out_channels= 64, kernel_size=5, stride=1),#\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.fc_layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(64 * 20 * 20, 120),\n",
    "            torch.nn.Linear(120, 49)\n",
    "        )\n",
    "        \n",
    "        # Spatial transformer localization-network\n",
    "        self.localization = torch.torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 8, kernel_size=7),\n",
    "            torch.nn.MaxPool2d(2, stride=2),\n",
    "            torch.nn.ReLU(True),\n",
    "            torch.nn.Conv2d(8, 10, kernel_size=5),\n",
    "            torch.nn.MaxPool2d(2, stride=2),\n",
    "            torch.nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        # Regressor for the 3 * 2 affine matrix\n",
    "        self.fc_loc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(10 * 3 * 3, 32),\n",
    "            torch.nn.ReLU(True),\n",
    "            torch.nn.Linear(32, 3 * 2)\n",
    "        )\n",
    "\n",
    "        # Initialize the weights/bias with identity transformation\n",
    "        self.fc_loc[2].weight.data.zero_()\n",
    "        self.fc_loc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n",
    "\n",
    "        \n",
    "    def stn(self, x):\n",
    "        xs = self.localization(x)\n",
    "        xs = xs.view(-1, 10 * 3 * 3)\n",
    "        theta = self.fc_loc(xs)\n",
    "        theta = theta.view(-1, 2, 3)\n",
    "\n",
    "        grid = F.affine_grid(theta, x.size())\n",
    "        x = F.grid_sample(x, grid)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.stn(x)\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc_layers(x)\n",
    "        X = F.softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "    \n",
    "    \n",
    "class ConvNet2(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "            # torch.nn.conv2d(in_channels, out_channels, kernel_size)\n",
    "            # in_channels is the number of layers which it takes in (i.e.num color channels in 1st layer)\n",
    "            # out_channels is the number of different filters that we use\n",
    "            # kernel_size is the depthxwidthxheight of the kernel#\n",
    "            # stride is how many pixels we shift the kernel by each time\n",
    "        self.cnn1 = torch.nn.Conv2d(in_channels=1, out_channels=32, kernel_size=5, stride=1) #out 32x24x24\n",
    "        self.cnn2 = torch.nn.Conv2d(in_channels=32, out_channels= 64, kernel_size=5, stride=1) #out 64x20x20\n",
    "        self.fc1 = torch.nn.Linear(64 * 20 * 20, 120)\n",
    "        self.fc2 = torch.nn.Linear(120, 49)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.cnn1(x))\n",
    "        x = F.relu(self.cnn2(x))\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "use_cuda = False\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN = ConvNet().to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "#criterion = torch.nn.BCELoss()\n",
    "# SET UP TRAINING VISUALISATION\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter() # we will use this to show our models performance on a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs, optimiser='Adam', learning_rate = 0.0001, verbose=False, tag='Loss/Train'):\n",
    "    if(optimiser=='Adam'):\n",
    "        optimiser = torch.optim.Adam(model.parameters(), lr= learning_rate)\n",
    "    else:\n",
    "        optimiser = torch.optim.SGD(model.parameters(), lr= learning_rate)\n",
    "        \n",
    "    \n",
    "    model.train()                                  # put the model into training mode (more on this later)\n",
    "    for epoch in range(epochs):\n",
    "        for idx, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            #print(inputs.shape)\n",
    "            #print(labels.shape)\n",
    "        \n",
    "            \n",
    "            prediction = model(inputs) # pass the data forward through the model\n",
    "            #print(prediction.shape)\n",
    "            #print('lab.type:', type(labels), '  pred.type:', type(prediction))\n",
    "            \n",
    "            #print(labels)\n",
    "            #break\n",
    "            loss = criterion(prediction, labels) # compute the loss\n",
    "            #if verbose: print('Epoch:', epoch, '\\tBatch:', idx, '\\tLoss:', loss)\n",
    "            optimiser.zero_grad() # reset the gradients attribute of each of the model's params to zero\n",
    "            loss.backward() # backward pass to compute and set all of the model param's gradients\n",
    "            optimiser.step() # update the model's parameters\n",
    "            writer.add_scalar(tag, loss, epoch*len(train_loader) + idx)    # write loss to a graph\n",
    "        if verbose: print('Epoch:', epoch, '\\tAccuracy:', calc_accuracy(model, val_loader), '\\tLoss:', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "            \n",
    "def calc_accuracy(model, dataloader, testset=False):\n",
    "    misclass_i = []\n",
    "    misclass_l = []\n",
    "    num_correct = 0\n",
    "    num_examples = len(dataloader.dataset)                       # test DATA not test LOADER\n",
    "    for inputs, labels in dataloader:                  # for all exampls, over all mini-batches in the test dataset\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        predictions = model(inputs)\n",
    "        predictions = torch.max(predictions, axis=1)    # reduce to find max indices along direction which column varies\n",
    "        predictions = predictions[1]                    # torch.max returns (values, indices)\n",
    "        num_correct += int(sum(predictions == labels))\n",
    "\n",
    "        if testset: #generate a list of misclassified data\n",
    "            if sum(predictions != labels):\n",
    "                for i in range(len(predictions)):\n",
    "                    if(predictions[i] != labels[i]):\n",
    "                        misclass_i.append(inputs[i])\n",
    "                        misclass_l.append(labels[i])\n",
    "                \n",
    "    percent_correct = num_correct / num_examples * 100\n",
    "    return percent_correct, misclass_i, misclass_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tAccuracy: (88.18785725320562, [], []) \tLoss: tensor(0.3839, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tAccuracy: (91.41356403522323, [], []) \tLoss: tensor(0.3764, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tAccuracy: (92.93990421751892, [], []) \tLoss: tensor(0.0724, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tAccuracy: (93.72470261084504, [], []) \tLoss: tensor(0.1173, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tAccuracy: (94.38591070600958, [], []) \tLoss: tensor(0.1190, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tAccuracy: (94.3333848292909, [], []) \tLoss: tensor(0.1911, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tAccuracy: (94.57129615325198, [], []) \tLoss: tensor(0.1334, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tAccuracy: (95.19233740151398, [], []) \tLoss: tensor(0.1645, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cnn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-123-942640588c11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimiser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train Accuracy:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcalc_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test Accuracy:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcalc_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cnn' is not defined"
     ]
    }
   ],
   "source": [
    "model = CNN\n",
    "\n",
    "train(model, epochs=8, optimiser='Adam', learning_rate=0.0001, verbose=True)\n",
    "print('Train Accuracy:', calc_accuracy(model, train_loader))\n",
    "print('Test Accuracy:', calc_accuracy(model, test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
